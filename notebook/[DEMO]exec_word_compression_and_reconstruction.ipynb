{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os, io\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"[Work Directory]\"\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model_template.losses import squared_error\n",
    "from model_template.noise_layers import GumbelNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path to the original embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_w2v = \"[path to the original word2vec embedding by gensim]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### path to the output model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_out_model = \"[path to the output model by Keras]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load w2v model and keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_w2v = Word2Vec.load(path_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(path_out_model, custom_objects={\"squared_error\":squared_error, \"GumbelNoise\":GumbelNoise})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SandBox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_layer = model.get_layer(name=\"input_x\")\n",
    "output_layer = model.get_layer(name=\"gumbel_softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_m = len(output_layer._outbound_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_tensor = [input_layer.input, K.learning_phase()]\n",
    "output_tensor = [output_layer.get_output_at(i) for i in range(N_m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_core = K.function(inputs=input_tensor, outputs=output_tensor)\n",
    "def encoder(vec_original):\n",
    "    PREDICTION_PHASE = 0\n",
    "    if vec_original.ndim == 1:\n",
    "        vec_original = np.expand_dims(vec_original, axis=0)\n",
    "    lst_vec_w_enc = encoder_core(inputs=[vec_original, PREDICTION_PHASE])\n",
    "    w_enc = np.array(map(np.argmax, lst_vec_w_enc)) # apply argmax for all vectors\n",
    "    return w_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_decoder_layer = [model.get_layer(name=\"decoder_%d\" % i) for i in range(N_m)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arry_decoder_params = np.array([layer.get_weights()[0] for layer in lst_decoder_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(vec_encoded):\n",
    "    ret = np.zeros(arry_decoder_params.shape[-1])\n",
    "    for m, k in enumerate(vec_encoded):\n",
    "        ret += arry_decoder_params[m,k]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test with arbitrary wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORDS = \"男,女,王,女王\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_word = WORDS.split(\",\")\n",
    "lst_word = [word for word in lst_word if word in model_w2v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dispaly original embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_vec_w = [model_w2v[word] for word in lst_word]\n",
    "pd.DataFrame(index=lst_word, data=lst_vec_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode specified words into code-book representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_vec_w_enc = [encoder(vec_w) for vec_w in lst_vec_w]\n",
    "pd.DataFrame(index=lst_word, data=lst_vec_w_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decode(=restore) code-book representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_vec_w_dec = [decoder(vec_w_enc) for vec_w_enc in lst_vec_w_enc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How similar between original embedding and restored ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_x, vec_y):\n",
    "    vec_x /= np.sqrt(np.sum(vec_x**2))\n",
    "    vec_y /= np.sqrt(np.sum(vec_y**2))\n",
    "    return np.sum(vec_x*vec_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lst_sim = [cosine_similarity(vec_w, vec_w_dec) for vec_w, vec_w_dec in zip(lst_vec_w, lst_vec_w_dec)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(index=lst_word, data={\"similarity\":lst_sim})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### can we compute word similarity in code-book dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discrete_similarity(vec_x, vec_y):\n",
    "    n_dim = vec_x.size\n",
    "    n_match = (vec_x == vec_y).sum()\n",
    "    return n_match / n_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_word = len(lst_word)\n",
    "mat_sim = np.array([discrete_similarity(vec_w1, vec_w2) for vec_w1, vec_w2 in product(lst_vec_w_enc, lst_vec_w_enc)]).reshape((n_word, n_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.DataFrame(index=lst_word, data=mat_sim)\n",
    "df_.columns = lst_word\n",
    "df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compare with word similarity in original space..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_w = np.vstack(lst_vec_w)\n",
    "mat_sim = mat_w.dot(mat_w.T)\n",
    "df_ = pd.DataFrame(index=lst_word, data=mat_sim)\n",
    "df_.columns = lst_word\n",
    "df_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
